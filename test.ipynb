{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import utils\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_instructions = utils.heart2jinst(context, questions)\n",
    "utils.jdump(heart_instructions, 'heart_data.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = 'facebook/opt-125m'\n",
    "cache_dir: str = None\n",
    "optim: str = \"adamw_torch\"\n",
    "model_max_length: int = 512\n",
    "data_args = train.DataArguments(data_path='heart_data.json')\n",
    "\n",
    "# make tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "        model_max_length=model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "data_module = train.make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    2, 45943,    16,    41, 15741,    14,  7448,    10,  3685,     6,\n",
       "         11153,    19,    41,  8135,    14,  1639,   617,  5377,     4, 21062,\n",
       "            10,  1263,    14, 16574, 25830,     5,  2069,     4, 50118, 50118,\n",
       "         48134, 41241,    35, 50118, 10836,    47,  6190,     5,  2511,     9,\n",
       "             5, 12925, 11328, 10607,  1761,   116, 50118, 50118, 48134, 41327,\n",
       "            35, 50118, 45152,   108, 14691, 13373,   128, 17779, 11328, 33425,\n",
       "          3934,   128,  6423, 13373,   128, 22117, 18429,  5423,   108, 24303,\n",
       "         50118, 50118, 48134, 19121,    35, 40022,  1253,     4,  1439,    16,\n",
       "            10,   220,  2706, 10607,  1761,    14,  3291,   561,    41, 10764,\n",
       "             9,  7193,   198,     5,   304,     9,  4687,     7,  1045,    23,\n",
       "          3189,     4,     2]),\n",
       " 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100, 40022,  1253,     4,  1439,    16,\n",
       "            10,   220,  2706, 10607,  1761,    14,  3291,   561,    41, 10764,\n",
       "             9,  7193,   198,     5,   304,     9,  4687,     7,  1045,    23,\n",
       "          3189,     4,     2])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module['train_dataset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Formatting inputs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    2, 45943,    16,    41, 15741,    14,  7448,    10,  3685,     4,\n",
       "        21062,    10,  1263,    14, 16574, 25830,     5,  2069,     4, 50118,\n",
       "        50118, 48134, 41241,    35, 50118, 31033,   130,  4965,    13,  4959,\n",
       "         2245,     4, 50118, 50118, 48134, 19121,    35,   134,     4, 43800,\n",
       "           10,  9320,  5626,     8,   146,   686,     7,   680,  2710,     9,\n",
       "        12849,     8,  8942,     4,  1437, 50118,   176,     4, 30450,  4595,\n",
       "            7,   489,   110,   809,  2171,     8,   670,     4,  1437, 50118,\n",
       "          246,     4,  2315,   615,  3581,     8,  3014,    10,  4292,  3581,\n",
       "         3078,     4,     2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.warning(\"Formatting inputs...\")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "\n",
    "list_data_dict = utils.jload('alpaca_data_3000.json')\n",
    "\n",
    "sources = [\n",
    "    prompt_input.format_map(example) if example.get(\n",
    "        \"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "    for example in list_data_dict\n",
    "]\n",
    "sources\n",
    "targets = [\n",
    "    f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "\n",
    "# logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "data_dict = train.preprocess(sources, targets, tokenizer)\n",
    "sources[0].split(' ')\n",
    "#targets[0]\n",
    "len(data_dict['input_ids'][0])\n",
    "input_ids = data_dict[\"input_ids\"]\n",
    "labels = data_dict[\"labels\"]\n",
    "input_ids[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
